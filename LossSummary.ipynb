{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dice loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Implementation with [TensorFlow / Keras](image_segmentation.ipynb)\n",
    "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced. \n",
    "\n",
    "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 [Implementation with Pytorch](https://github.com/pytorch/pytorch/issues/1249#issuecomment-305088398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:**\n",
    "\n",
    "Hi @IssamLaradji\n",
    "I've a few questions about the code.\n",
    "\n",
    "Does smooth similar to eps which avoid division by zero?\n",
    "Like the cross entropy loss, the result should be a positive value so I'm wondering if is that correct :\n",
    "`return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))`\n",
    "\n",
    "Thanks\n",
    "\n",
    "**A:**\n",
    "\n",
    "@tommy-qichang\n",
    "\n",
    "1. smooth does more than that. You can set smooth to zero and add eps to the denominator to prevent division by zero. However, having a larger smooth value (also known as Laplace smooth, or Additive smooth) can be used to avoid overfitting. The larger the smooth value the closer the following term is to 1 (if everything else is fixed), `((2. * intersection + smooth) /  (iflat.sum() + tflat.sum() + smooth))` This decreases the penalty obtained from having `2*intersection different from iflat.sum() + tflat.sum()`. A similar approach is commonly used in Naive Bayes, see equation (119) in these [notes](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "2. Yah that should be the case, good catch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [lovasz losses](LovaszSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
